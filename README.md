# Data_Management_Fundermental_Assessment
# Air Quality Analysis using (MySQL)
This project is divided into 5 components as see below;
## INTRODUCTION
This assessment was to test our knowledge in relational database management, I ensured to attend all classes, gaining the necessary knowledge to complete each component. Weekly lessons provided essential insights into tackling the tasks. My first step was carefully studying the brief to understand each part's requirements. I will be reflecting on COMPONENT 1 (ER Diagram and Database Design) Challenges Encountered: Designing the Entity-Relationship (ER) Diagram and determining the required tables was a significant challenge. Structuring the database and populating tables initially proved difficult. I identified three key tables: Constituency, Station, and Reading (AirQualityMeasurement), along with an optional Schema table for visualization. Overcoming Challenges/Experience Gained: Understanding the relationships between tables was crucial. Constituencies have many stations, and each station has multiple readings, forming a one-to-many relationship. The Constituency ID became a foreign key in the Station table. However, the Constituency table had four rows, while the Station table had 19 rows. Using Google Maps, I assigned each station to its constituency by searching its coordinates (latitude and longitude). This revealed that Bristol West has 11 stations, Bristol South has 4, Bristol North West has 2, and Bristol East has 1. With this information, I successfully created the ER Diagram.

## COMPONENT 2 (Forward Engineer the ER Model) 
While designing my ER model, I generated an SQL script by forward engineering it. This approach was simpler and less error-prone compared to manually writing the script. I then moved on to cropping and cleaning my data. Conceptual models tie together many ideas to explain a phenomenon or event.

## COMPONENT 3 (Cropping the Dataset)
Challenges Encountered: Cropping the Air Quality dataset for data from January 1, 2015, to October 23, 2023, was challenging. When I initially tried cleaning the data by removing rows with missing values, the entire dataset disappeared. Using Python to crop the dataset was intentional as I wanted hands-on experience and extra marks. However, after running the script, my result differed from my classmates'. The original dataset had 1,603,492 rows, while my cropped version had 400,000 rows, raising concerns about the script’s accuracy. Overcoming Challenges/Experience Gained: On closer inspection, I noticed some columns had very few or no values. I adjusted my focus to cropping the dataset for values from January 1, 2015, onwards and cleaning it to ensure it captured data up to October 23, 2023. I re-ran the script and obtained row count of 517,245. The earlier issue stemmed from not allowing the cropping process to complete due to an interruption.

## COMPONENT 4 (Importing Tables into the Database)
Manually importing the Constituency, Station, and Schema tables into phpMyAdmin was straightforward due to their small size. However, importing the large Reading table required Python. An alternative approach would have ben to just create a csv file for each of the table using excel then just import straight to phpMyAdmin using the import, Challenges Encountered: When creating the ER Diagram, the Site_ID foreign key in the Reading table was renamed to Station_Site_ID and moved to the last column. In the cropped dataset, it appeared as the second column, causing a mismatch. This created errors while i was running the python script which took a long while to import. Overcoming Challenges/Experience Gained: I edited the cropped dataset, moving the Site_ID column to the end, and updated my Python script to rename it to Station_Site_ID. This alignment ensured consistency between the dataset and the schema. This experience improved my skills in data manipulation and importing large datasets. The time it took to import was based on the volume and velocity of the dataset. 

## COMPONENT 5 (Querying the Data)
Querying the data was manageable due to the exercises provided during class and resources like W3Schools. For query-a, I identified the highest value of NO for about eight stations using JOIN and GROUP BY functions. For query-b, I calculated mean values of PM2.5 for AURN St Paul’s and Parson Street School but limited the output to prevent crashes. I applied the same method to the third query, ensuring efficiency and accuracy.

## Key Takeaway
This assessment taught me database design, time-series data manipulation, and Python-based database management. I realized the importance of data integrity and enrichment tools like Google Maps as it helped to determine which stations belonged to which constituency.

## CONCLUSION
Despite initial challenges, I completed the tasks successfully by exploring multiple methods and seeking clarity during classes. This experience enhanced my understanding of database systems and Python, preparing me for future projects. Screen Captures of all my results are located in the files.

# Air Quality Analysis using NoSQL (MongoDB and BaseX)

## Introduction
The reason for this component was to expose me in the use of NoSQL database in data managegment This part of the assessment turned out to be one of the most challenging yet interesting. After realizing that my Machine(MacBook Air Version 11.7.10) couldnt install most of the NoSql tools, i started becoming very frustratated, trying to install brew, but kept having errors and interuption due to my machine's version, having completed the rest components of the assessment, i was left with component 5(NoSql). This was very frustrating for me.

I was able to solve all of these challenges by beconing on a friend who's machine was able to perform some of the NoSql database This experience taught me how important it is to have a machine that has the capacity to handle all the softwares that are needed in for handling Data Science.

Experience and Challenge with MongoDB I had always thought of using MongoDB for the component 5 because i had always heard my tutor (Prakash), constantly refer to it so i assumed it's his favourite, although we where taught BaseX in class, but i was always drawn to MongoDB because of sentiments, couopled with the fact that trying something new won't be a bad idea as it will broaden my knowledge of NoSQL. I started up by picking my sample data which was the first 10 rows of my data set and then i designed it using the JSON script, this worked with python on Jupyter note book as i was able to create the Sample data structure, before this, i had to first of all import the library that would enable me connect the python to MongoDB and the name of the library i used was called pymongo library, and then imported it directly to MongoDB, the process was a long one, because i had to manualy populate my three tables; station table, constituency table and Reading table(AirQualityMeasurement). To represent a dataset in a NoSQL database, it is often useful to arrange data in aggregates. Each aggregate is a group of related application objects, representing a unit of data access and atomic manipulation (Atzeni et al., 2020). With this, i was able to create and structure my sample data. I choose one of the stations called Colston Avenue, which has 501 as its stationID, went back to my reading(AirQualitymeasurement) and used the first 10 data with 501 as its stationID, this was done manually because the sample data wasnt large. After reflection, i have come to realise that another approach to this would have been to create an XML file containing the Sample Data. After successfully importing the sample dataset into MongoDb, it was now time for me to run some queries which i couldnt. I tried all i could, but all effort proved abortive. Lets just say i couldn't run query on MongoDB. The frustration i got from not being able to run a query on MongoDB, led me to BaseX.

Experience with BaseX The challenges i encountered in MongoDB then made me resort to BaseX which turned out to be the easiest and seemless way to model data. What i did was rewrite the sample data that was initially written in JSON script and turn it to XML, because BaseX performs better with XML file and this process was very seemless, after importing the XML file carrying my sample data, i then started querying the sample data just so i could see how it works and it did worked as expected. i carried out 3 queries just to see how it works. My first query was to determine the elements with NO that are less than 20, the second query was to also check for NO2 that are greater than 100 in the sample data and finally we checked for NOx that are greater than or equal to 300. Screen captures of the query and it's out put are attached for better understanding.

Lessons Learnt This excersie made me understand the importance of Modeling and querying data using NoSQL. Chen and Lee 2019 discussed that enterprise’s database is desired to be accessed as fast as possible. To obtain complex information from multiple relations, RDB sometimes needs to perform SQL join operations to merge two or more relations at the same time, which can lead to performance bottlenecks. Besides, except the relational data storage format, other data storage formats have been proposed in many applications, such as key-value pairs, document-oriented, time series, etc. As a result, more and more enterprises have decided to use NoSQL databases to store big data. This shows that NoSQL is faster than RDM and also more straught forward, Chen and Lee 2019 also disusse that NoSQL is Schema-free, Unlike RDBs need to define database schema before inserting data, NoSQL databases do not need to do this. Therefore, NoSQL databases can flexibly add data. With the few knowledge i have gathered about NoSQL based on this assessment, i can confidently put on my CV that i am a NoSQL expert. Going forward, i will try to use NoSQl on random dataset practice.

Reference Atzeni, P., Bugiotti, F., Cabibbo, L., & Torlone, R.(2020). Data modeling in the NoSQL world. Computer Standards & Interfaces, 67, 103149. http://doi.org/10.1016/j.csi.2016.10.003

Chen, J.-K. & Lee, W.-Z., 2019. An Introduction of NoSQL Databases Based on Their Categories and Application Industries. Algorithms, 12(5), p.106. Available at: https://doi.org/10.3390/a12050106 [Accessed 13 Jan. 2025].
